{
  
    
        "post0": {
            "title": "Stroke Prediction Analysis",
            "content": "Overview . Welcome to my project on developing a stroke classification model. Data Science in the medical field allows for insights that can potentially save lives. The reason I choose this project is because of my aspirations to work as a data scientist in the medical field. The dataset taken is from Kaggle, however the user did not provide a source, therefore this project&#39;s intended usage is for education purposes only. In this project, I will be using the packages Pandas, Sklearn, Matplotlib. I will be utilizing random forest and neural networks to acheive the highest accuracy. The glossary for the columns are listed at the end of the page. . Link to Dataset: https://www.kaggle.com/fedesoriano/stroke-prediction-dataset . Link to Webmd Article: https://www.webmd.com/stroke/news/20181220/1-in-4-people-over-25-will-be-hit-by-stroke#:~:text=Rates%20vary%20country%20to%20country%2C%20but%20in%20the,sciences%20at%20the%20University%20of%20Washington%2C%20in%20Seattle. . import pandas as pd from sklearn.model_selection import train_test_split from sklearn.tree import DecisionTreeClassifier, export_graphviz, DecisionTreeRegressor from sklearn.preprocessing import OneHotEncoder from sklearn import tree from sklearn.ensemble import RandomForestClassifier from sklearn.preprocessing import StandardScaler,LabelEncoder from sklearn.pipeline import Pipeline from sklearn.metrics import confusion_matrix import matplotlib.pyplot as plt import seaborn as sns %matplotlib inline sns.set_style(&#39;whitegrid&#39;) . . Read Dataset . stroke = pd.read_csv(&quot;~/Downloads/Stroke_Data.csv&quot;) stroke.head() . . id gender age hypertension heart_disease ever_married work_type Residence_type avg_glucose_level bmi smoking_status stroke . 0 9046 | Male | 67.0 | 0 | 1 | Yes | Private | Urban | 228.69 | 36.6 | formerly smoked | 1 | . 1 51676 | Female | 61.0 | 0 | 0 | Yes | Self-employed | Rural | 202.21 | NaN | never smoked | 1 | . 2 31112 | Male | 80.0 | 0 | 1 | Yes | Private | Rural | 105.92 | 32.5 | never smoked | 1 | . 3 60182 | Female | 49.0 | 0 | 0 | Yes | Private | Urban | 171.23 | 34.4 | smokes | 1 | . 4 1665 | Female | 79.0 | 1 | 0 | Yes | Self-employed | Rural | 174.12 | 24.0 | never smoked | 1 | . Data Cleaning . The dataset contains 201 null values that are all located in the bmi column. There are two ways to handle null values, we can discard them from the dataset or insert values. In this case, I have elected to insert values because of the lack of data available in the dataset. The null values represent 3.93% of the total rows in the dataset. We could just insert the average bmi value dependent on gender and age. However, it would be a perfect time to use a decision tree to find bmi values using age and gender as my predictors. Since, I am basing my project on the webmd article indicating one in four people over 25 years old suffer a stroke, I will be filtering the data to all records above 25. After excluding people under 25 in our dataset we have a total of 3817 records. . Percentage of Null Values: 3.93% . Text(0.5, 1.0, &#39;Heatmap of Null Values&#39;) . id gender age hypertension heart_disease ever_married work_type Residence_type avg_glucose_level bmi smoking_status stroke . 1 51676 | Female | 61.0 | 0 | 0 | Yes | Self-employed | Rural | 202.21 | NaN | never smoked | 1 | . 8 27419 | Female | 59.0 | 0 | 0 | Yes | Private | Rural | 76.15 | NaN | Unknown | 1 | . 13 8213 | Male | 78.0 | 0 | 1 | Yes | Private | Urban | 219.84 | NaN | Unknown | 1 | . Decision Tree . DT_bmi_pipe = Pipeline(steps=[(&#39;scale&#39;,StandardScaler()),(&#39;lr&#39;,DecisionTreeRegressor(random_state=101))]) X = stroke[[&#39;age&#39;,&#39;gender&#39;,&#39;bmi&#39;]].copy() X.gender = X.gender.replace({&#39;Male&#39;:0,&#39;Female&#39;:1,&#39;Other&#39;:-1}).astype(np.uint8) Missing_Values = X[X.bmi.isna()] X = X[~X.bmi.isna()] Y = X.pop(&#39;bmi&#39;) DT_bmi_pipe.fit(X,Y) predicted_bmi = pd.Series(DT_bmi_pipe.predict(Missing_Values[[&#39;age&#39;,&#39;gender&#39;]]),index=Missing_Values.index) stroke.loc[Missing_Values.index,&#39;bmi&#39;] = round(predicted_bmi,1) . id gender age hypertension heart_disease ever_married work_type Residence_type avg_glucose_level bmi smoking_status stroke . 1 51676 | Female | 61.0 | 0 | 0 | Yes | Self-employed | Rural | 202.21 | 29.9 | never smoked | 1 | . 8 27419 | Female | 59.0 | 0 | 0 | Yes | Private | Rural | 76.15 | 30.6 | Unknown | 1 | . 13 8213 | Male | 78.0 | 0 | 1 | Yes | Private | Urban | 219.84 | 27.2 | Unknown | 1 | . stroke = stroke[(stroke[&#39;age&#39;] &gt; 25)] stroke.shape . (3817, 12) . EDA . Initially we are going to look at visualizations that provide context to our dataset. The first visualization presented showcases the different genders in the dataset. We have 2,296 females (60.15%), 1,520 males (39.80%), and 1 other (0.02%). . stroke.gender.value_counts().plot(kind = &#39;pie&#39;,wedgeprops = { &#39;linewidth&#39; : 1, &#39;edgecolor&#39; : &#39;black&#39; }, colors=[&#39;pink&#39;,&#39;lightblue&#39;]) . . &lt;AxesSubplot:ylabel=&#39;gender&#39;&gt; . Female: 2296 records, 60.15% Male: 1520 records, 39.82% Other: 1 record, 0.03% . stroke_age_y = stroke[stroke[&#39;stroke&#39;] == 1][&#39;age&#39;] stroke_age_n = stroke[stroke[&#39;stroke&#39;] == 0][&#39;age&#39;] plt.hist([stroke_age_y, stroke_age_n], bins = 15, stacked = True) plt.legend([&#39;Stroke = Yes&#39;, &#39;Stroke = No&#39;]) plt.title(&#39;Histogram of Age with Stroke Overlay&#39;) plt.xlabel(&#39;Age&#39;) plt.ylabel(&#39;Frequency&#39;) plt.show() . . stroke[&#39;bmi_binning&#39;] = pd.cut(x = stroke.bmi, bins = [0, 18.5, 25, 30, 35, 40, 100], labels = [&#39;Underweight&#39;, &#39;Healthy&#39;, &#39;Overweight&#39;,&#39;Class 1 Obesity&#39;, &#39;Class 2 Obesity&#39;,&#39;Severe Obesity&#39;], right = False) . crosstab_05 = pd.crosstab(stroke[&#39;bmi_binning&#39;],stroke[&#39;stroke&#39;]) crosstab_norm_05 = crosstab_05.div(crosstab_05.sum(1), axis = 0) print(crosstab_norm_05) . stroke 0 1 bmi_binning Underweight 0.966667 0.033333 Healthy 0.950071 0.049929 Overweight 0.926598 0.073402 Class 1 Obesity 0.926357 0.073643 Class 2 Obesity 0.950538 0.049462 Severe Obesity 0.949868 0.050132 . crosstab_06 = pd.crosstab(stroke[&#39;hypertension&#39;],stroke[&#39;stroke&#39;]) crosstab_norm_06 = crosstab_06.div(crosstab_06.sum(1), axis = 0) print(crosstab_norm_06) . stroke 0 1 hypertension 0 0.946481 0.053519 1 0.865854 0.134146 . crosstab_07 = pd.crosstab(stroke[&#39;heart_disease&#39;],stroke[&#39;stroke&#39;]) crosstab_norm_07 = crosstab_07.div(crosstab_07.sum(1), axis = 0) print(crosstab_norm_07) . stroke 0 1 heart_disease 0 0.944429 0.055571 1 0.829091 0.170909 . stroke[&#39;avg_glucose_binning&#39;] = pd.cut(x = stroke.avg_glucose_level, bins = [0, 140, 200, 500], labels = [&#39;Normal&#39;, &#39;Prediabetes&#39;, &#39;Diabetes&#39;], right = False) . crosstab_09 = pd.crosstab(stroke[&#39;avg_glucose_binning&#39;],stroke[&#39;stroke&#39;]) crosstab_norm_09 = crosstab_09.div(crosstab_09.sum(1), axis = 0) print(crosstab_norm_09) . stroke 0 1 avg_glucose_binning Normal 0.950704 0.049296 Prediabetes 0.886154 0.113846 Diabetes 0.868235 0.131765 . Machine Learning . stroke_train, stroke_test = train_test_split(stroke, test_size = 0.25, random_state = 7) . print(&#39;Training Dataset Size:&#39;,stroke_train.shape) print(&#39;Test Dataset Size:&#39;,stroke_train.shape) . Training Dataset Size: (2905, 15) Test Dataset Size: (2905, 15) . print(&#39;Baseline Model&#39;,round((stroke_train.stroke == 0).mean(),2)) . Baseline Model 0.93 . print(&#39;Baseline Model&#39;,round((stroke_test.stroke == 0).mean(),2)) . Baseline Model 0.94 . Sample Size . x = pd.DataFrame(stroke.groupby([&#39;stroke&#39;])[&#39;stroke&#39;].count()) # plot fig, ax = plt.subplots(figsize = (6,6), dpi = 70) ax.barh([1], x.stroke[1], height = 0.7, color = &#39;lightgrey&#39;) plt.text(-1150,-0.08, &#39;Healthy&#39;,{&#39;font&#39;: &#39;Times New Roman&#39;,&#39;weight&#39;:&#39;bold&#39;,&#39;Size&#39;: &#39;16&#39;,&#39;style&#39;:&#39;normal&#39;, &#39;color&#39;:&#39;#512b58&#39;}) plt.text(5000,-0.08, &#39;95%&#39;,{&#39;font&#39;:&#39;Times New Roman&#39;,&#39;weight&#39;:&#39;bold&#39; ,&#39;size&#39;:&#39;16&#39;,&#39;color&#39;:&#39;#512b58&#39;}) ax.barh([0], x.stroke[0], height = 0.7, color = &#39;black&#39;) plt.text(-1000,1, &#39;Stroke&#39;, {&#39;font&#39;: &#39;Times New Roman&#39;,&#39;weight&#39;:&#39;bold&#39;,&#39;Size&#39;: &#39;16&#39;,&#39;style&#39;:&#39;normal&#39;, &#39;color&#39;:&#39;#fe346e&#39;}) plt.text(300,1, &#39;5%&#39;,{&#39;font&#39;:&#39;Times New Roman&#39;, &#39;weight&#39;:&#39;bold&#39;,&#39;size&#39;:&#39;16&#39;,&#39;color&#39;:&#39;#fe346e&#39;}) fig.patch.set_facecolor(&#39;#f6f5f5&#39;) ax.set_facecolor(&#39;#f6f5f5&#39;) plt.text(-1150,1.77, &#39;Percentage of People Having Strokes&#39; ,{&#39;font&#39;: &#39;Times New Roman&#39;, &#39;Size&#39;: &#39;25&#39;,&#39;weight&#39;:&#39;bold&#39;, &#39;color&#39;:&#39;black&#39;}) plt.text(-1150,1.5, &#39;It is a highly unbalanced distribution, nand clearly seen that 5 in 100 people are susceptible nto heart strokes.&#39;, {&#39;font&#39;:&#39;Times New Roman&#39;, &#39;size&#39;:&#39;12.5&#39;,&#39;color&#39;: &#39;black&#39;}) ax.axes.get_xaxis().set_visible(False) ax.axes.get_yaxis().set_visible(False) ax.spines[&#39;bottom&#39;].set_visible(False) ax.spines[&#39;left&#39;].set_visible(True) ax.spines[&#39;right&#39;].set_visible(False) ax.spines[&#39;top&#39;].set_visible(False) . &lt;ipython-input-75-c5045eea6f5b&gt;:6: MatplotlibDeprecationWarning: Case-insensitive properties were deprecated in 3.3 and support will be removed two minor releases later plt.text(-1150,-0.08, &#39;Healthy&#39;,{&#39;font&#39;: &#39;Times New Roman&#39;,&#39;weight&#39;:&#39;bold&#39;,&#39;Size&#39;: &#39;16&#39;,&#39;style&#39;:&#39;normal&#39;, &#39;color&#39;:&#39;#512b58&#39;}) &lt;ipython-input-75-c5045eea6f5b&gt;:9: MatplotlibDeprecationWarning: Case-insensitive properties were deprecated in 3.3 and support will be removed two minor releases later plt.text(-1000,1, &#39;Stroke&#39;, {&#39;font&#39;: &#39;Times New Roman&#39;,&#39;weight&#39;:&#39;bold&#39;,&#39;Size&#39;: &#39;16&#39;,&#39;style&#39;:&#39;normal&#39;, &#39;color&#39;:&#39;#fe346e&#39;}) &lt;ipython-input-75-c5045eea6f5b&gt;:15: MatplotlibDeprecationWarning: Case-insensitive properties were deprecated in 3.3 and support will be removed two minor releases later plt.text(-1150,1.77, &#39;Percentage of People Having Strokes&#39; ,{&#39;font&#39;: &#39;Times New Roman&#39;, &#39;Size&#39;: &#39;25&#39;,&#39;weight&#39;:&#39;bold&#39;, &#39;color&#39;:&#39;black&#39;}) . stroke_train.stroke.value_counts()/len(stroke_train) . 0 0.933907 1 0.066093 Name: stroke, dtype: float64 . stroke_train.stroke.value_counts() . 0 2713 1 192 Name: stroke, dtype: int64 . q = 0.26 oversampling_size = (int)(((2713+192)*q - 192)/(1 - q)) print(oversampling_size) to_resample = stroke_train[stroke_train[&#39;stroke&#39;] == 1] # sampling with replacement our_resample = to_resample.sample(n = oversampling_size, replace = True) # to concatenate two DataFrames stroke_train_rebal = pd.concat([stroke_train, our_resample]) . 761 . stroke_train_rebal[&#39;stroke&#39;].value_counts() . 0 2713 1 953 Name: stroke, dtype: int64 . stroke_train = stroke_train_rebal stroke_train.shape . (3666, 15) . stroke_train.reset_index(inplace = True) stroke_test.reset_index(inplace = True) . CART Method . enc = OneHotEncoder(handle_unknown = &#39;ignore&#39;) enc.fit(pd.DataFrame(stroke_train.smoking_status)) smoking_status_train_enc = pd.DataFrame(enc.transform(pd.DataFrame( stroke_train.smoking_status)).toarray(), columns = enc.get_feature_names([&#39;smoking_status&#39;])) stroke_train = pd.concat([stroke_train, smoking_status_train_enc], axis = 1) smoking_status_test_enc = pd.DataFrame(enc.transform(pd.DataFrame( stroke_test.smoking_status)).toarray(), columns = enc.get_feature_names([&#39;smoking_status&#39;])) stroke_test = pd.concat([stroke_test, smoking_status_test_enc], axis = 1) . y_names = [&#39;No Stroke&#39;, &#39;Stroke&#39;] predictors = [&#39;age&#39;,&#39;hypertension&#39;,&#39;heart_disease&#39;,&#39;avg_glucose_level&#39;,&#39;bmi&#39;] predictors.extend(smoking_status_train_enc.columns.to_list()) print(predictors, y_names) . [&#39;age&#39;, &#39;hypertension&#39;, &#39;heart_disease&#39;, &#39;avg_glucose_level&#39;, &#39;bmi&#39;, &#39;smoking_status_Unknown&#39;, &#39;smoking_status_formerly smoked&#39;, &#39;smoking_status_never smoked&#39;, &#39;smoking_status_smokes&#39;] [&#39;No Stroke&#39;, &#39;Stroke&#39;] . X_train = stroke_train[predictors] y_train = stroke_train[&#39;stroke&#39;] X_test = stroke_test[predictors] y_test = stroke_test[&#39;stroke&#39;] . cart01 = DecisionTreeClassifier(criterion = &quot;gini&quot;, max_leaf_nodes = 5).fit( X_train, y_train) fig, axes = plt.subplots(nrows = 1,ncols = 1,figsize = (4,4), dpi=300) tree.plot_tree(cart01, feature_names = predictors, class_names=y_names, filled = True); plt.show() . pred_train = cart01.predict(X_train) # compute the accuracy rate on the training data print((pred_train == y_train).mean()) # confusion matrix conf_m = pd.DataFrame(confusion_matrix(y_train, pred_train), index = [&#39;N&#39;, &#39;P&#39;], columns = [&#39;n&#39;, &#39;y&#39;]) conf_m . 0.7749590834697218 . n y . N 2237 | 476 | . P 349 | 604 | . pred_test = cart01.predict(X_test) # compute the accuracy rate on the test data print((pred_test == y_test).mean()) # confusion matrix conf_m = pd.DataFrame(confusion_matrix(y_test, pred_test), index = [&#39;N&#39;, &#39;P&#39;], columns = [&#39;n&#39;, &#39;y&#39;]) conf_m . 0.7915376676986584 . n y . N 737 | 177 | . P 25 | 30 | . Random Forests . rfy = np.ravel(stroke_train.stroke) # Random Forest rf_01 = RandomForestClassifier(n_estimators = 100, criterion = &#39;gini&#39;).fit(X_train, rfy) . pred_test = rf_01.predict(X_test) # compute the accuracy rate on the test data print(&quot;Accuracy:&quot;, round((pred_test == y_test).mean(),2)) . Accuracy: 0.93 . Link to Resample: https://www.webmd.com/stroke/news/20181220/1-in-4-people-over-25-will-be-hit-by-stroke#:~:text=Rates%20vary%20country%20to%20country%2C%20but%20in%20the,sciences%20at%20the%20University%20of%20Washington%2C%20in%20Seattle. Link to Age Category: https://academic.oup.com/gerontologist/article/42/1/92/641498 Link to Glucose Category: https://www.mayoclinic.org/diseases-conditions/diabetes/diagnosis-treatment/drc-20371451 .",
            "url": "https://dbaldessari66.github.io/resume-website/sklearn/logistical%20regression/random%20forests/data%20cleaning/python/neural%20network/decision%20trees/2021/09/22/Stroke-Prediction.html",
            "relUrl": "/sklearn/logistical%20regression/random%20forests/data%20cleaning/python/neural%20network/decision%20trees/2021/09/22/Stroke-Prediction.html",
            "date": " • Sep 22, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Video Game Recommendation Engine",
            "content": "Overview . Welcome to my project on creating a video game recommendation system. Many streaming services utilize recommendation systems to increase customer engagement with their platform. I wanted to create a similar system for video games to display new games for users to play. In this project, we will be using a content-based recommender system. Therefore, we will base our recommendations on titles, publishers, descriptions, genres, and tags that different items share. During this project, I will be utilizing the packages Pandas, Numpy, and Sklearn. These are all standard packages for data manipulation, mathematics, and machine learning applications. . Link for Dataset: https://www.kaggle.com/trolukovich/steam-games-complete-dataset . import pandas as pd import numpy as np from sklearn.metrics.pairwise import cosine_similarity from sklearn.feature_extraction.text import CountVectorizer . Games = pd.read_csv(&#39;~/Downloads/steam_games 2.csv&#39;) . Background . The dataset features 20 columns, many that will not be of use to this type of recommendation system. As well, there are 40,833 unique video games with unique characteristics. The recommendation system is designed to suit the needs of novice gamers. Therefore, we will be excluding free games and focusing on Triple-A titles. Triple-A games are video games produced or developed by a major publisher, which allocated a large budget for both development and marketing. Many novice gamers will be familiar with Triple-A games rather than small indie games. Most Triple-A titles retail price is $59.99, however, some games release months or years after their console release to the steam platform for a discount. Therefore we will limit our dataset to only titles with a price range of $19.99 to $59.99. . Games.head(3) . url types name desc_snippet recent_reviews all_reviews release_date developer publisher popular_tags game_details languages achievements genre game_description mature_content minimum_requirements recommended_requirements original_price discount_price . 0 https://store.steampowered.com/app/379720/DOOM/ | app | DOOM | Now includes all three premium DLC packs (Unto... | Very Positive,(554),- 89% of the 554 user revi... | Very Positive,(42,550),- 92% of the 42,550 use... | May 12, 2016 | id Software | Bethesda Softworks,Bethesda Softworks | FPS,Gore,Action,Demons,Shooter,First-Person,Gr... | Single-player,Multi-player,Co-op,Steam Achieve... | English,French,Italian,German,Spanish - Spain,... | 54.0 | Action | About This Game Developed by id software, the... | NaN | Minimum:,OS:,Windows 7/8.1/10 (64-bit versions... | Recommended:,OS:,Windows 7/8.1/10 (64-bit vers... | $19.99 | $14.99 | . 1 https://store.steampowered.com/app/578080/PLAY... | app | PLAYERUNKNOWN&#39;S BATTLEGROUNDS | PLAYERUNKNOWN&#39;S BATTLEGROUNDS is a battle roya... | Mixed,(6,214),- 49% of the 6,214 user reviews ... | Mixed,(836,608),- 49% of the 836,608 user revi... | Dec 21, 2017 | PUBG Corporation | PUBG Corporation,PUBG Corporation | Survival,Shooter,Multiplayer,Battle Royale,PvP... | Multi-player,Online Multi-Player,Stats | English,Korean,Simplified Chinese,French,Germa... | 37.0 | Action,Adventure,Massively Multiplayer | About This Game PLAYERUNKNOWN&#39;S BATTLEGROUND... | Mature Content Description The developers de... | Minimum:,Requires a 64-bit processor and opera... | Recommended:,Requires a 64-bit processor and o... | $29.99 | NaN | . 2 https://store.steampowered.com/app/637090/BATT... | app | BATTLETECH | Take command of your own mercenary outfit of &#39;... | Mixed,(166),- 54% of the 166 user reviews in t... | Mostly Positive,(7,030),- 71% of the 7,030 use... | Apr 24, 2018 | Harebrained Schemes | Paradox Interactive,Paradox Interactive | Mechs,Strategy,Turn-Based,Turn-Based Tactics,S... | Single-player,Multi-player,Online Multi-Player... | English,French,German,Russian | 128.0 | Action,Adventure,Strategy | About This Game From original BATTLETECH/Mec... | NaN | Minimum:,Requires a 64-bit processor and opera... | Recommended:,Requires a 64-bit processor and o... | $39.99 | NaN | . Step One: Filtering the price . The original price column will be the column we intend to filter. We have a problem to sort out before we proceed with our filtering. We cannot sort the original price column because it is not considered a numerical type. We can fix this by first converting the column to a character type, then remove the dollar sign through character string slicing. After we remove the dollar sign, we can convert the column to a numerical type. Now we can proceed with applying the filter. The total number of unique games in the dataset is now 4,338. . Games.original_price . 0 $19.99 1 $29.99 2 $39.99 3 $44.99 4 Free ... 40828 $2.99 40829 $2.99 40830 $7.99 40831 $9.99 40832 $4.99 Name: original_price, Length: 40833, dtype: object . Games[&#39;original_price&#39;] = Games[&#39;original_price&#39;].str[1:] . Games[&#39;original_price&#39;] = pd.to_numeric(Games[&#39;original_price&#39;],errors=&#39;coerce&#39;) . Games = Games[(Games[&#39;original_price&#39;] &gt;= 19.99) &amp; (Games[&#39;original_price&#39;] &lt;= 59.99)] . Games.shape . (4338, 20) . Step Two: Choosing columns to use in the recommendation system . When choosing which columns to put in the recommendation system, we should be mindful of the characteristics gamer&#39;s value. The developer variable is important to include since developers often have the same team working on different games. Therefore each game produced by the same developer will have a similar style of gameplay. Genre variable provides a broad grouping of games with similarities in form, style, or subject matter. Popular Tags variable is an in-depth description of different gaming characteristics. The Game Details variable lists a game&#39;s online offering such as whether a game is single-player or multiplayer. The last variable would be the name of the game, which is valuable because sequels and prequels will be included in the recommendation. . Games.head(3) . url types name desc_snippet recent_reviews all_reviews release_date developer publisher popular_tags game_details languages achievements genre game_description mature_content minimum_requirements recommended_requirements original_price discount_price . 0 https://store.steampowered.com/app/379720/DOOM/ | app | DOOM | Now includes all three premium DLC packs (Unto... | Very Positive,(554),- 89% of the 554 user revi... | Very Positive,(42,550),- 92% of the 42,550 use... | May 12, 2016 | id Software | Bethesda Softworks,Bethesda Softworks | FPS,Gore,Action,Demons,Shooter,First-Person,Gr... | Single-player,Multi-player,Co-op,Steam Achieve... | English,French,Italian,German,Spanish - Spain,... | 54.0 | Action | About This Game Developed by id software, the... | NaN | Minimum:,OS:,Windows 7/8.1/10 (64-bit versions... | Recommended:,OS:,Windows 7/8.1/10 (64-bit vers... | 19.99 | $14.99 | . 1 https://store.steampowered.com/app/578080/PLAY... | app | PLAYERUNKNOWN&#39;S BATTLEGROUNDS | PLAYERUNKNOWN&#39;S BATTLEGROUNDS is a battle roya... | Mixed,(6,214),- 49% of the 6,214 user reviews ... | Mixed,(836,608),- 49% of the 836,608 user revi... | Dec 21, 2017 | PUBG Corporation | PUBG Corporation,PUBG Corporation | Survival,Shooter,Multiplayer,Battle Royale,PvP... | Multi-player,Online Multi-Player,Stats | English,Korean,Simplified Chinese,French,Germa... | 37.0 | Action,Adventure,Massively Multiplayer | About This Game PLAYERUNKNOWN&#39;S BATTLEGROUND... | Mature Content Description The developers de... | Minimum:,Requires a 64-bit processor and opera... | Recommended:,Requires a 64-bit processor and o... | 29.99 | NaN | . 2 https://store.steampowered.com/app/637090/BATT... | app | BATTLETECH | Take command of your own mercenary outfit of &#39;... | Mixed,(166),- 54% of the 166 user reviews in t... | Mostly Positive,(7,030),- 71% of the 7,030 use... | Apr 24, 2018 | Harebrained Schemes | Paradox Interactive,Paradox Interactive | Mechs,Strategy,Turn-Based,Turn-Based Tactics,S... | Single-player,Multi-player,Online Multi-Player... | English,French,German,Russian | 128.0 | Action,Adventure,Strategy | About This Game From original BATTLETECH/Mec... | NaN | Minimum:,Requires a 64-bit processor and opera... | Recommended:,Requires a 64-bit processor and o... | 39.99 | NaN | . Games = Games[[&#39;genre&#39;,&#39;game_details&#39;,&#39;popular_tags&#39;,&#39;developer&#39;,&#39;name&#39;]] . Step Three: Drop all rows with null values . Usually, the first step in any project would be to eliminate null values. However, it is important to wait to perform this step. We have previously consolidated columns to only useful columns for the recommendation system. Now that the dataset only has useful columns, we can eliminate only rows where null values are present in the columns we have chosen. After eliminating null values the total unique games in the dataset are 3,999. We will also be adding a new column labeled Game_ID, which provides a numerical unique value to each game. . Games.head(3) . genre game_details popular_tags developer name . 0 Action | Single-player,Multi-player,Co-op,Steam Achieve... | FPS,Gore,Action,Demons,Shooter,First-Person,Gr... | id Software | DOOM | . 1 Action,Adventure,Massively Multiplayer | Multi-player,Online Multi-Player,Stats | Survival,Shooter,Multiplayer,Battle Royale,PvP... | PUBG Corporation | PLAYERUNKNOWN&#39;S BATTLEGROUNDS | . 2 Action,Adventure,Strategy | Single-player,Multi-player,Online Multi-Player... | Mechs,Strategy,Turn-Based,Turn-Based Tactics,S... | Harebrained Schemes | BATTLETECH | . Games.dropna(inplace = True) . Games.shape . (3999, 5) . Games[&#39;Game_ID&#39;] = range(0,3999) . Games.isnull().values.any() . False . Games = Games.reset_index() . Step Four: Combine selected column&#39;s values into string . Our next step is going to be creating a function that compiles all data in each column selected into one giant string. In order to do so, we are going to make an empty list called important features and then append the values of the desired columns. Then we create a column called important features, where we call the function on the dataset. . def get_important_features(data): important_features = [] for i in range(0, data.shape[0]): important_features.append(data[&#39;name&#39;][i]+&#39; &#39;+data[&#39;developer&#39;][i]+&#39; &#39;+data[&#39;popular_tags&#39;][i]+&#39; &#39;+data[&#39;genre&#39;][i]+data[&#39;game_details&#39;][i]) return important_features . Games[&#39;important_features&#39;] = get_important_features(Games) Games.important_features.head(3) . 0 DOOM id Software FPS,Gore,Action,Demons,Shoote... 1 PLAYERUNKNOWN&#39;S BATTLEGROUNDS PUBG Corporation... 2 BATTLETECH Harebrained Schemes Mechs,Strategy,... Name: important_features, dtype: object . Step Five: Assemble similarity matrix . First, we will be using the count vectorizer function to transform a given text into a vector. The matrix consists of a frequency of words in a string. For example the string &#39;Action, Action, Adventure&#39;, the matrix will display a table with the word, Action, and a frequency of two. Then we can use the cosine similarity function to measure the correlation among the different games. This function produces a matrix with the correlations between each game. The matrix contains a numerical value from zero to one, where a variable closer to one is considered a good recommendation, and a variable closer to zero is considered a poor recommendation. The diagonal line of the value one showcases a perfect correlation because it is the same game on each axis. . cm = CountVectorizer().fit_transform(Games[&#39;important_features&#39;]) . cs = cosine_similarity(cm) . print(cs) . [[1. 0.40406102 0.44932255 ... 0.4276686 0.18002057 0.19738551] [0.40406102 1. 0.34163336 ... 0.41871789 0.31520362 0.26363719] [0.44932255 0.34163336 1. ... 0.26702293 0.27136386 0.33377867] ... [0.4276686 0.41871789 0.26702293 ... 1. 0.35533453 0.27272727] [0.18002057 0.31520362 0.27136386 ... 0.35533453 1. 0.07106691] [0.19738551 0.26363719 0.33377867 ... 0.27272727 0.07106691 1. ]] . Step Six: Use the Recommendation System . Our last step would be to enter the name of the game we wish to get recommendations from. In this case, I have chosen the game Doom Eternal. We then create a new object called title_id, where we obtain the Game_ID value for Doom Eternal, which we assigned to each title in the third step. After this step, we are going to create a list of enumerations that contain the similarity score between each game and Doom Eternal. Then we sort the similarity score in descending order to receive the games with the highest similarities to Doom Eternal. I have chosen to display the top seven games that are recommended to us based on the characteristics of Doom Eternal. . title = &#39;DOOM Eternal&#39; title_id = Games[Games.name == title][&#39;Game_ID&#39;].values[0] . scores = list(enumerate(cs[title_id])) . sorted_scores = sorted(scores, key = lambda x:x[1], reverse = True) sorted_scores = sorted_scores[1:] . j = 0 print(&#39;The 7 most recommended games to&#39;, title, &#39;are: n&#39;) for item in sorted_scores: game_title = Games[Games.Game_ID == item[0]][&#39;name&#39;].values[0] print(j+1, game_title) j = j+1 if j &gt; 6: break . The 7 most recommended games to DOOM Eternal are: 1 Doom 3: BFG Edition 2 DOOM 3 Dead Space™ 2 4 DUSK 5 Max Payne 3 6 Unreal Tournament 3 Black 7 Crysis 2 - Maximum Edition . Conclusion . When observing the top seven results we can see the similarities between the games. The more similarities in each column the higher the ranking will be. For instance, Doom 3: BFG Edition and DOOM have similarities in every column. While the bottom four recommendations have values in common in the genre, game details, and popular tags columns. From my personal experience playing five out of the seven recommended games, I would like to have these games recommended to me based on my interest of DOOM Eternal. . Games = Games.set_index(&#39;name&#39;) . Games.loc[[&#39;DOOM Eternal&#39;,&#39;Doom 3: BFG Edition&#39;,&#39;DOOM&#39;,&#39;Dead Space™ 2&#39;,&#39;DUSK&#39;,&#39;Max Payne 3&#39;,&#39;Unreal Tournament 3 Black&#39;,&#39;Crysis 2 - Maximum Edition&#39;], [&#39;genre&#39;,&#39;game_details&#39;,&#39;popular_tags&#39;,&#39;developer&#39;]] . genre game_details popular_tags developer . name . DOOM Eternal Action | Single-player,Multi-player,Online Multi-Player... | Gore,Violent,Action,FPS,Great Soundtrack,Demon... | id Software | . Doom 3: BFG Edition Action | Single-player,Multi-player,Steam Achievements,... | FPS,Horror,Action,Shooter,Classic,Sci-fi,Singl... | id Software | . DOOM Action | Single-player,Multi-player,Co-op,Steam Achieve... | FPS,Gore,Action,Demons,Shooter,First-Person,Gr... | id Software | . Dead Space™ 2 Action | Single-player,Multi-player,Partial Controller ... | Horror,Action,Sci-fi,Space,Third Person,Surviv... | Visceral Games | . DUSK Action,Indie | Single-player,Online Multi-Player,Steam Achiev... | FPS,Retro,Action,Fast-Paced,Great Soundtrack,H... | David Szymanski | . Max Payne 3 Action | Single-player,Multi-player,Steam Achievements,... | Action,Third-Person Shooter,Bullet Time,Story ... | Rockstar Studios | . Unreal Tournament 3 Black Action | Single-player,Multi-player,Co-op,Steam Achieve... | FPS,Action,Multiplayer,Arena Shooter,Shooter,S... | Epic Games, Inc. | . Crysis 2 - Maximum Edition Action | Single-player,Partial Controller Support | Action,FPS,Sci-fi,Shooter,Singleplayer,Multipl... | Crytek Studios | .",
            "url": "https://dbaldessari66.github.io/resume-website/sklearn/nlp/recommendation%20engine/data%20cleaning/python/2021/09/21/Video-Game-Recommendation.html",
            "relUrl": "/sklearn/nlp/recommendation%20engine/data%20cleaning/python/2021/09/21/Video-Game-Recommendation.html",
            "date": " • Sep 21, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Pandemic Investing",
            "content": "Investing In A Pandemic . Dylan Baldessari 2020-12-25 . Introduction . Welcome to my financial analytic project, which I have used to find the stock with the best fundamentals in and coming out of the pandemic. I will be using a variety of metrics to seek this goal; Multiples, Projected Sales Growth, Revenue, Market Cap, Net Margin, and Returns. Since this project is based on investing throughout the pandemic, we will be using the time frame of the March lows (3/23/2020) and the Pfizer vaccine approval date (11/18/2020). I choose the March low 3/23/20 since this is generally the bottom for most equities as this was the moment the FED and U.S. Government was able to put the floor in the stock market through fiscal policy and monetary policy. Of course, this project has a bias toward the tail end of the time frame as the market is often irrational and forward-thinking, therefore cyclical sectors are likely to outperform secular growth sectors. Now, the pandemic is still going on unfortunately and it is likely to be around in the near-term future. However, this time frame captures the life-cycle of an investor’s preferences during the different stages of a pandemic. Therefore, if we spike in cases we will see a similar trend towards the beginning of our research, where we see secular sectors outperform, and as cases trend lower with vaccines rolling out we will see cyclical sectors outperform. We are likely to see similar trends take place in the future, as we have seen in the sample of time taken in this project and that’s why I feel comfortable with the time frame. I will be walking you through the steps taken to find this perfect stock. As a disclaimer, this “perfect stock” does not mean the best returning stock. As we see stocks with no revenues providing great returns in this current market. But, we wish to find a risk-neutral investor the perfect stock during and coming out of the pandemic. In doing so, the strongest fundamental stock will oftentimes lead to outperformance of the market benchmark (S&amp;P 500) and its peers. . Definitions of Financial Terms: . Market Cap / Revenue (TTM): A form of valuation multiple used to judge what investors are willing to pay for revenues. . Projected Sales Growth: A consensus of Analyst’s sales forecasts for next year’s revenues. . Market Cap: Total dollar market value of all outstanding shares. . Revenue: The income generated from normal business operations. . Net Margin: A ratio of net profits to revenues for a company. . First Step: Read in First Data Source . I have decided to not include the code related to loading in packages as it takes up too much space and we want to get right into the content. However, I will provide the packages included here (Tidyverse, Readxl, Ggplot2, Dplyr, Plotly, Ggrepel, Quantmod). These packages allow me to read, manipulate, and visualize data in different ways. The first data source I will be reading into R is the performance of several sector SPDR ETFs from the March lows to the Vaccine Approval date. I took this data from Yahoo Finance. . #Read in Data Set ETF_Performance &lt;- read_excel(&quot;~/Downloads/ETF_Performance.xlsx&quot;, col_types = c(&quot;text&quot;, &quot;numeric&quot;, &quot;numeric&quot;, &quot;skip&quot;, &quot;skip&quot;, &quot;skip&quot;, &quot;skip&quot;, &quot;skip&quot;, &quot;skip&quot;, &quot;skip&quot;, &quot;skip&quot;, &quot;skip&quot;)) print(ETF_Performance) . ## # A tibble: 11 × 3 ## Sectors `3_23_2020` `11_18_2020` ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 XLC 40.3 63.6 ## 2 XLY 87.5 154. ## 3 XLP 48.6 66.7 ## 4 XLE 27.4 35.4 ## 5 XLF 17.7 28.9 ## 6 XLV 74.6 109. ## 7 XLI 47.7 87.2 ## 8 XLB 38.4 69.5 ## 9 XLRE 25.5 37.1 ## 10 XLK 70.4 120. ## 11 XLU 44.9 64.2 . Second Step: See Which Sector Performed the Best . Next, we will need to do some cleaning and data manipulation to display the total return for each ETF from March 23rd to November 18th. We will be creating a new column called Total_Return that displays this information. Then we will need to utilize the function (sprintf) to format the column in a percentage format. This step will allow us to be able to input correct data for our graph. . #Calculate the total return from March 3rd, 2020 to November 18th, 2020 ETF &lt;- ETF_Performance %&gt;% rename(March_Low = `3_23_2020`, Vaccine_Approved = `11_18_2020`) %&gt;% mutate(Total_Return = (Vaccine_Approved - March_Low) / March_Low) %&gt;% mutate(Total_Return = sprintf(&quot;%0.1f%%&quot;, Total_Return * 100)) print(ETF) . ## # A tibble: 11 × 4 ## Sectors March_Low Vaccine_Approved Total_Return ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 XLC 40.3 63.6 57.8% ## 2 XLY 87.5 154. 75.6% ## 3 XLP 48.6 66.7 37.2% ## 4 XLE 27.4 35.4 29.5% ## 5 XLF 17.7 28.9 63.9% ## 6 XLV 74.6 109. 46.2% ## 7 XLI 47.7 87.2 82.8% ## 8 XLB 38.4 69.5 81.3% ## 9 XLRE 25.5 37.1 45.8% ## 10 XLK 70.4 120. 71.0% ## 11 XLU 44.9 64.2 42.8% . Since we have obtained the returns for each sector it is time to visualize the data to understand the best performing ETF. . #Visualize the Total Returns for the Sectors ggplot(ETF, aes(Sectors, Total_Return))+ geom_col(fill = &quot;light blue&quot;)+ geom_text(aes(label = Total_Return, vjust = -0.5))+ labs(y = &quot;Total Return&quot;, title = &quot;Comparison of Returns of Sector ETF&#39;s&quot;) . . We can now see the XLI (Industrial) sector was the best performing sector during this time frame. We can clearly see the sectors investors should have been investing in due to the pandemic conditions (XLK, XLB, XLY, XLI). Interestingly, these sectors all performed higher than 70% returns, however, the timing of the sectors performance is vastly different. . . We see the secular sectors outperform during the beginning and middle of the pandemic as it was a requirement to utilize these companies’ offerings. The XLK (Technology) and XLY (Consumer Discretionary) have trends such as E-Commerce, Digitization, Cloud, Essential Retailers, etc. These trends are fantastic investment trends going into the future and the pandemic pulled forward a lot of growth for these companies therefore, they outperformed. The latter half of the time frame was dominated by cyclical sectors like XLI (Industrials) and XLB (Materials) as the market focused its attention on the possible reopening of the economies across the world because of the vaccine hopes. These sectors have undergone a tremendous downturn in earnings and revenues due to the pandemic, but with the economy reopening it will allow for higher revenue growth as these company’s operations revert to their mean results. . The XLI is the best performing sector in this time frame with an 82.8% return. The Industrial sector has a lot of tailwinds as the economy is likely to bounce back and more stimulus is provided for potential infrastructure projects. The market’s recent rotation into cyclical sectors may be in the early innings going into 2021. We will take a look at multiples and other sector fundamentals to understand this rotation. . Third Step: Comparing Companies in the XLI Based on Sales Growth and Net Margin . Now we will be analyzing each company that makes up this sector to understand the underlying fundamentals of the sector. I will be neglecting to include companies with a weighting in the ETF lower than 1%. This is because most of these smaller weighted companies have smaller market caps, which results in low volume/float companies. In order to find the perfect investment, liquidity is a must, so the stock does not have drastic swings. I have gathered the information from ETF.com and Yahoo Finance. In this chunk of code, I have created a new column that represents a multiple, so we can compare the valuations of each company. . #Read in Data portfolio_holdings_xli &lt;- read_excel(&quot;~/Downloads/portfolio-holdings-xli.xls&quot;) #Create a Valuation metric column of MKT Cap/Rev (TTM) XLI_Real &lt;- portfolio_holdings_xli %&gt;% mutate(Multiple = Market_Cap / Revenue_TTM) %&gt;% mutate(Multiple = round(Multiple, 2)) head(data.frame(XLI_Real)) . ## Symbol Sales_Growth Net_Margin Market_Cap Revenue_TTM March_Low ## 1 UPS 0.048 0.0571 150.1 80.3 91.90 ## 2 RTX 0.057 -0.0314 107.5 59.7 51.72 ## 3 GE 0.023 0.0471 95.3 83.9 6.11 ## 4 FDX 0.048 0.0328 72.0 74.7 111.76 ## 5 ETN 0.003 0.0756 46.8 18.4 57.77 ## 6 NOC 0.047 0.0694 50.4 35.3 269.86 ## Vaccine_Approval Return Variable Multiple ## 1 168.45 0.8329706 1 1.87 ## 2 69.51 0.3439675 1 1.80 ## 3 9.73 0.5924714 1 1.14 ## 4 282.50 1.5277380 1 0.96 ## 5 116.75 1.0209451 1 2.54 ## 6 310.60 0.1509672 1 1.43 . Our first visualization for this data table will feature net margin and projected sales growth for each company. We will be breaking up these two metrics in the form of quartiles, the horizontal and vertical line will represent the median for the metrics among the companies in the ETF. In theory, the highest multiple or highest investor sentiment stock should come from quartile 4. However, in recent years the stock market has valued revenue growth over profitability. Therefore we will look closely at the companies in quartile 2 as they will most likely obtain the highest return among other quartiles. . #Visualize XLI Holdings based on Sales Growth and Net Margin options(ggrepel.max.overlaps = Inf) ggplot(XLI_Real, aes(Net_Margin, Sales_Growth, color = Symbol))+ geom_point(size = 1)+ scale_y_continuous(labels = scales::percent)+ scale_x_continuous(labels = scales::percent)+ geom_vline(xintercept = median(XLI_Real$Net_Margin))+ geom_hline(yintercept = median(XLI_Real$Sales_Growth))+ labs(x = &quot;Net Margin&quot;, y = &quot;Sales Growth&quot;, title = &quot;Comparison of Growth vs. Profitability&quot;)+ geom_text_repel(data = XLI_Real, size = 2.5, mapping = aes(x = `Net_Margin`, y = `Sales_Growth`, label = Symbol))+ geom_text(label = &quot;Q1&quot;, x = -0.081, y = -0.002, size = 3.5, color = &quot;black&quot;)+ geom_text(label = &quot;Q2&quot;, x = -0.081, y = 0.337, size = 3.5, color = &quot;black&quot;)+ geom_text(label = &quot;Q3&quot;, x = 0.299, y = -0.002, size = 3.5, color = &quot;black&quot;)+ geom_text(label = &quot;Q4&quot;, x = 0.299, y = 0.337, size = 3.5, color = &quot;black&quot;) . . We do have to keep in mind the effects of Covid-19, many of these companies’ net margin has taken a hit due to the pandemic slowing down operations. Therefore, we can assume by next year many of these companies can increase profitability. For example, PCAR can obtain a multiple expansion among investors if the net margin were to raise higher than the median of the sector and the stock price will rise. In doing this analysis on other sectors, we often see many quartile 2 companies are young companies because revenue/earnings have yet to mature, however when the company scales it is hard to maintain above-average sales growth, therefore companies must focus on achieving profitability to maintain multiples. In this case, many of the high growth companies are purely bounce back plays as we are coming out of lockdown. In order to justify high multiples, these companies must shift attention to profitability. . Step 4: Find the Highest Returning Quartile . Next, we will be looking at the returns of each quartile to see which quartile produced the best returns, high growth, high profitability, or a mix of both? . #Median for Return XLI_Median &lt;- XLI_Real %&gt;% group_by(Variable) %&gt;% summarise(median_val = median(Return)) #Visualize Returns of each quartile ggplot() + geom_col(data = XLI_Real, aes(x = Symbol, y = Return, fill = Variable)) + scale_y_continuous(labels = scales::percent)+ facet_wrap(~Variable, ncol = 1, scales = &quot;free_x&quot;)+ geom_hline(data = XLI_Median, aes(yintercept = median_val))+ labs(title = &quot;Quartiles Return from March Low to Vaccine Approval&quot;) . . Our theory on the market valuing growth over profitability is correct, with quartile 2 having the highest return of 102%. It is interesting to see quartile 4, which has both high growth and high profitability come in last with 76% returns. quartile 1 and quartile 3 come in at 83% and 93% respectively. I have used median over the mean as these quartiles can be skewed by one of two stocks, therefore median mitigates the risk of one stock skewing the data with its returns. . Step 5: Understanding Companies Valuation . We have found the highest returning quartile, which is quartile 2. Now let’s see if valuations in quartile 2 are overvalued and due for a reversion to fair value. . #Visualize Multiple with Sales Growth ggplot(XLI_Real, aes(Sales_Growth, Multiple, color = Variable))+ geom_point()+ scale_x_continuous(labels = scales::percent)+ labs(x = &quot;Sales Growth&quot;, title = &quot;Comparison of Multiples vs. Sales Growth of XLI&quot;)+ geom_text_repel(data = XLI_Real, size = 2.5, mapping = aes(x = `Sales_Growth`, y = `Multiple`, label = Symbol))+ geom_smooth(method = lm, se=FALSE) . ## `geom_smooth()` using formula &#39;y ~ x&#39; . . I took a linear regression of the group’s multiples and sales growth to visualize a relationship between variables. . If we look at each quartile, we will see investors are willing to place quartile 4 companies with the highest multiple. Multiples do not 100% correlate with returns as quartile 4 had the worst return. The multiple is just a way to compare what investors are willing to pay for one stock rather than another. An example would be two identical companies, Company A and Company B, with the same revenue and profit. However, Company A has a subscription model and Company B has a one-time charge model. Investors could place a higher multiple on Company A because of the reliability of earnings. But, what does correlate with returns is multiple expansion or contraction. If a company’s multiple is increasing while earnings are remaining the same, it is because the stock price is increasing and vise versa for a multiple decreasing. In this model, we see quartile 2 being undervalued if they can execute their projections of sales and improve profitability. . Step 6: Looking at Quartile 2’s Revenues . Our next graph will look at Q2 company’s revenue from 2016 to 2020. This is to see where revenues were in the past and see if there are any deceleration or acceleration. . #Read in Data Q_2 &lt;- read_excel(&quot;~/Downloads/Q_2.xlsx&quot;) #Take Highest Growth and Profitable Stocks and Chart Revenues ggplot(Q_2, aes(x = Quarter,y = Revenue, color = Ticker)) + geom_line()+ scale_color_manual(values=c(&quot;Green&quot;,&quot;Black&quot;,&quot;Yellow&quot;,&quot;Blue&quot;,&quot;Red&quot;,&quot;Purple&quot;,&quot;Orange&quot;)) . . The Pandemic begins to show its effects from Quarter 16 through the present day. Trough revenues for the group are Quarter 2 of 2020 (18th Quarter in Chart). We can see how these companies have been performing pre-pandemic and can utilize the data to project future revenues as we come out of the pandemic. . Step 7: Understanding The Revenues . I have chosen three companies based on their ability to get back to pre-covid revenue levels. As well I have taken in previous data such as multiple, previous returns, and the possibility for multiple expansion. . Boeing . Boeing’s Revenue Segments; Commercial Airplanes (67.71%), Precision Engagement &amp; Mobility Systems (15.5%), Network Systems (8.14%), Support System (8.31%), Boeing Capital Corporation (0.44%), Other (-0.1%). . Boeing’s revenues have been depressed over the past year because of the grounding of the 737 Max and the pandemic’s effect on travel. The 737 Max was grounded due to the unfortunate crashes in 2018 and 2019 that took many lives. During the 20-month hiatus of the 737 Max, the FAA and Boeing have made precautions to ensure the safety of Boeing’s passengers. The Pandemic has decimated travel across the world. For instance, if we look at TSA checkpoint travel numbers from 2020 and 2019, such as November 18th, 2019 had 2,071,631 total checkpoints but in 2020 on November 18th the total number was only 703,135. Compile these two headwinds together, we can see how Boeing’s Q3 revenues have declined by 29.23% year-over-year. But, fortunes for Boeing are starting to turn. Boeing on November 18th has gotten approval from the FAA for the 737 Max to fly again. As well, the vaccine being distributed will serve as a tailwind to air travel for the next few years as consumers have pent up demand to travel. These revenue drivers have already begun to show as Boeing is closing deals to sell aircrafts, which can justify a massive 33.30% projected sales growth for next year. . Caterpillar . Caterpillar’s Revenue Segments; Machinery (48.81%), Engines (41.04%), Financing &amp; Insurance (6.25%), Corporate items (-0.38%), All Other Segments (4.28%). . Caterpillar’s revenue is well diversified in many segments as well as many regions. The pandemic serves as a black swan event for Caterpillar as all of their revenues have been disrupted. Caterpillar is the leading equipment manufacture for industries like construction, mining, and oil and gas. As we know these were the hardest hit segments. Energy consumption has slowed during the pandemic as we continue to work from home and air travel has slowed. However, Caterpillar’s management team have handled the pandemic well, cutting costs quickly, and having dealerships begin to work off inventories as the production of new products slowed. Pre-Covid Caterpillar provided investors a stable revenue growth and is likely to bounce back to their peak sales. Caterpillar is a blue-chip company for a reason they have tremendous execution of selling their equipment worldwide. An expectation of an infrastructure bill early in 2021 under a Biden presidency gives Caterpillar a new catalyst to justify the 9.70% projected sales growth for next year. . Paccar . Paccar’s Revenue Segments; Truck (77.25%), Parts (16.05%), Financial Services (6.21%), Other (0.49%). . Paccar is an indirect way to play the E-commerce boom. As more and more orders flow into E-commerce rather than retail stores more delivery methods are needed. Paccar is a traditional heavy-duty truck manufacturer. During the pandemic, many companies were set on cost savings rather than purchasing new equipment. Paccar has struggled like most Industrials that rely on a strong economy to produce great results. Recently, Paccar has announced its plans to enter the electric/hydrogen vehicle market. Paccar announced they plan on beginning production of their battery-operated electric trucks in 2021. As companies are becoming more ESG based, it is a smart move to begin production of an ECO-friendlier product. Investor’s excitement for electric vehicles has been a tailwind for Paccar’s stock performance. As well the pent-up demand for trucks will be prevalent in 2021 as retail sales rebound and e-commerce continues to be a growth driver. These current trends in the industry can justify the 22.20% projected sales growth for next year. . Final Step: Technical Analysis . #Gather Stock Data Industrials&lt;- c(&quot;BA&quot;, &quot;CAT&quot;, &quot;PCAR&quot;) getSymbols(Industrials, src = &quot;yahoo&quot;, from = &quot;2020-03-23&quot;, to = &quot;2020-11-18&quot;) . ## [1] &quot;BA&quot; &quot;CAT&quot; &quot;PCAR&quot; . candleChart(BA, TA = &quot;addRSI();addSMA(n = 50)&quot;) . . Boeing in recent months has been trading in a channel between $200 and $140. The November run-up is due to the anticipation of approval from the FAA of the 737 Max. As we can see from technical indicators like the RSI and 50-day moving average BA is in overbought conditions. The RSI is approaching the overbought condition of 75 with a score of 73.07 and the 50-day moving average has remained relatively low compared to the stocks parabolic move. Boeing likely is to consolidate around the recent high in June and the $180 price level due to the consolidation in past months. . candleChart(CAT, TA = &quot;addRSI();addSMA(n = 50)&quot;) . . Caterpillar’s stock chart from the March low is a beautiful looking chart from a technical analysis standpoint. The chart has healthy pullbacks and consolidations followed by steady rises in stock price. The RSI would indicate Caterpillar is not overbought or oversold scoring . The stock has been trading in an upward channel and bounced off the 50-day moving average multiple times. It would be safe to consider the stock bouncing off the $160 price level as that is where the moving average indicated. | candleChart(PCAR, TA = &quot;addRSI();addSMA(n = 50)&quot;) . . Paccar has been a very stable investment, which comes with no surprise as the beta is 1.05. The stock has been in consolidation since July’s big move, trading between $95 on the top side and $80 on the bottom side. With consolidation comes an RSI at a neutral score of 49.12 and the 50-day moving average at current price levels. In order to break the trend-line, Paccar needs a catalyst either positive leading to the upside or negative leading into the downside. It is likely investors are waiting for upcoming earnings in 2021 to justify a stock move above previous highs. The stock is likely to remain in the channel unless a significant catalyst occurs. . Conclusion . Our analysis has brought our attention to three names; Boeing, Caterpillar, and Paccar. It is no shock that Boeing and Caterpillar make the list as they are Dow components and blue-chip companies that should be in a diversified portfolio. Now it’s the hard part, which one is the perfect stock for the pandemic and coming out of the pandemic. I would rate Caterpillar as the best stock to offer investors the most value on a fundamental basis. This pick has a great influence from previously mentioned fundamentals, balance sheet, management team, and risks. Boeing and Paccar have some serious threats associated with them. For Boeing, they have taken on $25 Billion in debt that was issued in May. This overhang of debt puts pressure on Boeing’s management team to execute operations. Boeing may be able to execute operations perfectly however, the real test is if passengers are willing to travel during the distribution of the vaccine and will passengers refuse to fly on the 737 Max. Paccar is not the perfect stock based on the rapid increase in competition in the electric vehicle space. Paccar must make the transition to electric vehicles and fast with Tesla and other EV companies focusing on the heavy-duty trucking industry. Paccar is keeping R&amp;D expense relatively the same as 2019, coming in at a range of $310-$340 million. The transition may cause Paccar to achieve extreme growth rates relative to their past, however, it will hinder profitability as they have to compete with a new wave of EV producers. Therefore, I am not choosing Paccar based on the moat shrinking. Caterpillar sets up to be the perfect stock among the three. If we look at revenues for Caterpillar it was stable growth and predictable, since they are diverse across the world. Caterpillar has shown in the past and during the pandemic, they have an excellent management team that can appease their investor’s expectations. Caterpillar has maintained their dividend, unlike Boeing, due to the financial handling of its balance sheet. As well, Caterpillar is capable of focusing attention on profitability as they will be selling new equipment that is beneficial to the bottom line and push Caterpillar into quartile 4, thus having Caterpillar’s multiple expanding, thus increasing the stock price. From a technical standpoint, Caterpillar sets up the best for a risk-neutral investor as they are very stable and provide the most value to investors. . Thank you for reading through my financial project, please leave any feedback in the comments below. .",
            "url": "https://dbaldessari66.github.io/resume-website/2020/12/25/Pandemic-Investing.html",
            "relUrl": "/2020/12/25/Pandemic-Investing.html",
            "date": " • Dec 25, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://dbaldessari66.github.io/resume-website/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://dbaldessari66.github.io/resume-website/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Hello my name is Dylan Baldessari . Contact Information . Aspirations . Data Science Professional with 2 years of experience learning and implementing machine learning techniques, visualizations, data cleaning and manipulation in several projects. My greatest strength is lifelong learning, which provides the ability to adapt to different situations. . Education . Work Experience . Technical Skills and Certifications . Leadership Roles .",
          "url": "https://dbaldessari66.github.io/resume-website/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://dbaldessari66.github.io/resume-website/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}